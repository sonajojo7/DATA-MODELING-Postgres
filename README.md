# Project Overview 

*The startup  Sparkify means to analyze the data they have been collected on songs and user activity on their new music streaming app. The analytics team is trying to understand what songs users are listening to.
Querying is difficult as the available data include a directory of JSON logs on user activity on the app and another directory with JSON metadata on the songs in the app.
A Postgres database is to be created with tables designed to optimize queries on song play analysis.We need to create a database schema and ETL pipeline for this analysis.* 



# Star Schema for the project

**Fact Table**
1. songplays - records in log data associated with song plays i.e. records with page NextSong
   songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
   
**Dimension Tables**

1. users - users in the app
   user_id, first_name, last_name, gender, level
2. songs - songs in music database
   song_id, title, artist_id, year, duration
3. artists - artists in music database
   artist_id, name, location, latitude, longitude
4. time - timestamps of records in songplays broken down into specific units
   start_time, hour, day, week, month, year, weekday
   
# The Process

**Create Tables**
1. Write CREATE statements in sql_queries.py to create each table.
2. Write DROP statements in sql_queries.py to drop each table if it exists.
3. Run create_tables.py in the console using the command !python create_tables.py in the console to create your database and tables.
4. Once the create_tables.py is run, run test.ipynb to confirm the creation of your tables with the correct columns. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.

**Build ETL Processes**

*Follow instructions in the etl.ipynb notebook to develop ETL processes for each table.*

*At the end of each table section, or at the end of the notebook, run test.ipynb to confirm that records were successfully inserted into each table.*

*If the running of cells in test.ipynb gives you the tables with correct column names and data, we can proced further.*

*Rerun create_tables.py to reset your tables before each time you run this notebook.*


# Files Provided
**DataSets**
- song_data -

*The first dataset is a subset of real data from the Million Song Dataset. Each file is made in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID*
- log_data -

*The second dataset consists of log files made in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.*

**Project Workspace Files**

1. test.ipynb
     *The notebook that displays the first few rows of each table to let you check your database.*
2. create_tables.py 
    *The script used to drop and create your tables. You run this file to reset your tables before each time you run your ETL scripts.*
3. etl.ipynb 
    *The notebook that reads and processes a single file from song_data and log_data and loads the data into your tables.* 
4. etl.py 
     *The script that reads and processes files from song_data and log_data and loads them into your tables.* 
5. sql_queries.py 
      *This is where  all your sql queries are made, and is imported into the last three files above.*
